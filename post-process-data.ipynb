{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read raw events from our feather files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "\n",
    "yaml = YAML(typ=\"safe\")\n",
    "\n",
    "with open(\"cfg.yaml\") as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def load_data():\n",
    "    dataframes = []\n",
    "    for parent, dirs, files in chain(\n",
    "        os.walk(\"data/id-type-created_at-repo.name-repo.url-actor.id-actor.login/\"),\n",
    "        os.walk(\"data/old\"),\n",
    "        os.walk(\"data/email\"),\n",
    "\n",
    "    ):\n",
    "        for fname in files:\n",
    "            if fname.endswith(\".feather\"):\n",
    "                path = os.path.join(parent, fname)\n",
    "                dataframes.append(pd.read_feather(path))\n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "\n",
    "raw_data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data a bit:\n",
    "\n",
    "Data in some older years has a different schema:\n",
    "\n",
    "- repo_name is just `nbviewer` not `jupyter/nbviewer`\n",
    "- only `repo_url` contains full org/name\n",
    "- `repo_url` may be either `https://github.com/jupyter/nbviewer` or `https://api.github.com/repos/jupyter/nbviewer`\n",
    "- older events may not have actor.id or event.id\n",
    "- actor.id is float, despite being integer data\n",
    "- event.id is str, despite being integer data\n",
    "- some events appear to be double-reported\n",
    "\n",
    "So:\n",
    "\n",
    "- drop some duplicate events\n",
    "- drop id field after removing duplicates\n",
    "- make repo_name consistent across schema changes\n",
    "- add org column from the first part of the repo_name\n",
    "- track some repos across renames\n",
    "- sort by date\n",
    "- backfill missing actor_id\n",
    "- fill still-missing actor_id with new unique values\n",
    "- ignore bot-initiated events\n",
    "- ignore fork/watch events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 270619/900298 duplicate events\n",
      "Dropped 5203 bot events\n"
     ]
    }
   ],
   "source": [
    "# some events may be reported multiple times in the data\n",
    "raw_rows = len(raw_data)\n",
    "df = raw_data.drop_duplicates()  # just on \"id\"? \"id\" is undefined for old data\n",
    "without_dupes = len(df)\n",
    "print(f\"Dropped {len(raw_data) - len(df)}/{len(raw_data)} duplicate events\")\n",
    "# drop uninteresting event-id columnt after removing duplicates\n",
    "df = df.drop(columns=[\"id\"])\n",
    "\n",
    "# drop bot events\n",
    "known_bots = [\n",
    "    \"travisbot\",\n",
    "    \"sourcegraphbot\",\n",
    "    \"jupyterlab-bot\",\n",
    "    \"npmcdn-to-unpkg-bot\",\n",
    "    \"codetriage-readme-bot\",\n",
    "    \"henchbot\",\n",
    "    \"lektor-bot\",\n",
    "]\n",
    "df = df[~(df.actor_login.str.endswith(\"[bot]\") | df.actor_login.isin(known_bots))]\n",
    "without_bots = len(df)\n",
    "\n",
    "print(f\"Dropped {without_dupes - without_bots} bot events\")\n",
    "\n",
    "# parse non-uniform repo.url into repo_name, repo_url\n",
    "df[\"repo_name\"] = df.repo_url.str.extract(\n",
    "    r\"https://[^/]+/(?:repos/)?(.+)\"\n",
    ")\n",
    "\n",
    "# apply repo renames\n",
    "for src, dest in config[\"renames\"].items():\n",
    "    df.loc[df.repo_name==src, \"repo_name\"] = dest\n",
    "\n",
    "# add org column\n",
    "df[\"org\"] = df.repo_name.str.split(\"/\", expand=True)[0]\n",
    "\n",
    "# drop any repos not in our current config\n",
    "# df = df[df.org.isin(config[\"orgs\"])]\n",
    "\n",
    "# sort by date\n",
    "df = df.sort_values(\"created_at\")\n",
    "\n",
    "# cast integer id columns to integers\n",
    "df[\"actor_id\"] = df[\"actor_id\"].astype(\"Int64\")\n",
    "df\n",
    "\n",
    "# more intuitive name for created_at\n",
    "df[\"date\"] = df[\"created_at\"]\n",
    "df.drop(columns=[\"repo_url\", \"created_at\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IssueCommentEvent                235480\n",
       "WatchEvent                        98047\n",
       "PullRequestEvent                  63690\n",
       "IssuesEvent                       56023\n",
       "PushEvent                         53909\n",
       "PullRequestReviewCommentEvent     35574\n",
       "ForkEvent                         35061\n",
       "EmailEvent                        25191\n",
       "PullRequestCommentEvent            8600\n",
       "CreateEvent                        4720\n",
       "GollumEvent                        2409\n",
       "DeleteEvent                        2178\n",
       "IssueEvent                         1468\n",
       "PullRequestReviewEvent              748\n",
       "CommitCommentEvent                  617\n",
       "MemberEvent                         465\n",
       "ReleaseEvent                        269\n",
       "PublicEvent                          16\n",
       "TeamAddEvent                         11\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back-fill actor ids from events with matching login that do have an id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103306/624476 events lacking actor id\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df.actor_id.isna().sum()}/{len(df)} events lacking actor id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_id_map = df.dropna(subset=[\"actor_id\"]).groupby(\"actor_login\").actor_id.first()\n",
    "df.loc[df[\"actor_id\"].isna(), \"actor_id\"] = df.actor_login[df.actor_id.isna()].map(\n",
    "    login_id_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31965/624476 events still lacking actor id\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df.actor_id.isna().sum()}/{len(df)} events still lacking actor id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, drop common but less interesting fork/watch events\n",
    "Do this after back-filling actor ids, since many of these events could be\n",
    "a source of actor_login:actor_id mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 133108/624476 fork/watch events\n",
      "26352/491368 events still lacking actor id\n"
     ]
    }
   ],
   "source": [
    "before = len(df)\n",
    "ignore_events = [\"ForkEvent\", \"WatchEvent\"]\n",
    "df = df[~df.type.isin(ignore_events)]\n",
    "\n",
    "print(f\"Dropped {before-len(df)}/{before} fork/watch events\")\n",
    "print(f\"{df.actor_id.isna().sum()}/{len(df)} events still lacking actor id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find actor_logins still without any actor_id and assign them new, unique ids\n",
    "use a counter starting just above the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled out remaining 26352 actor_ids\n"
     ]
    }
   ],
   "source": [
    "logins_without_id = df[df.actor_id.isna()].actor_login.unique()\n",
    "max_actor_id = df.actor_id.dropna().max()\n",
    "\n",
    "new_actor_ids = np.arange(max_actor_id + 1, max_actor_id + 1 + len(logins_without_id))\n",
    "actor_id_missing = df.actor_id.isna()\n",
    "\n",
    "new_actor_id_map = pd.Series(new_actor_ids, index=logins_without_id)\n",
    "df.loc[actor_id_missing, \"actor_id\"] = df.actor_login[actor_id_missing].map(new_actor_id_map)\n",
    "df.actor_id.isna().sum()\n",
    "print(f\"Filled out remaining {actor_id_missing.sum()} actor_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have actor ids for everyone and can use actor_id for all analysis\n",
    "instead of the non-unique actor_login (accounts can be renamed)\n",
    "\n",
    "We may have a small number of double-counts for some of the 6000 logins without actor id who may have renamed\n",
    "to a later login with an id.\n",
    "\n",
    "We also don't try to map EmailEvents onto their github counterparts, so active users on the ML may be double-counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actor_id\n",
       "890156          [jankatins, JanSchulz, janschulz]\n",
       "5635139      [MaximilianR, maxim-lian, max-sixty]\n",
       "10365377    [nottaanibot, thethomask, nymoorland]\n",
       "26246495      [NicolaiRiis, nicolairiis, nabriis]\n",
       "28781481            [soodooo, CandleSense, fsksf]\n",
       "Name: actor_login, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_login = df.dropna(subset=[\"actor_id\"]).groupby(\"actor_id\").actor_login\n",
    "login_counts = groupby_login.nunique()\n",
    "multiple_logins = login_counts[login_counts > 2]\n",
    "\n",
    "df[df.actor_id.isin(multiple_logins.index)].groupby(\"actor_id\").actor_login.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the post-processed data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_feather(\"data/processed.feather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
